#!/usr/bin/env python3
import argparse
import re
import statistics
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path

# Match href value (quoted or unquoted), then capture leading digits before first dot or end. Generated by AI.
_HREF_RE = re.compile(r'href\s*=\s*["\']?([^"\'>\s]+)', re.I)


def parse_html_links(html: str) -> list[int]:
    """Extract link IDs from hrefs (e.g. 123 or 123.html -> 123). Uses regex for speed."""
    ids = []
    for match in _HREF_RE.finditer(html):
        href = match.group(1)
        try:
            ids.append(int(href.split(".", 1)[0]))
        except ValueError:
            pass
    return ids


def build_graph(items: list, page_ids: set):
    adjacency_list = {}
    for page_id, outgoing in items:
        if page_id in page_ids:
            adjacency_list[page_id] = [t for t in outgoing if t in page_ids]
    return page_ids, adjacency_list


def build_reverse_adjacency(adjacency_list: dict, all_ids: set) -> dict[int, list[int]]:
    rev = {nid: [] for nid in all_ids}
    for src, targets in adjacency_list.items():
        for t in targets:
            rev[t].append(src)
    return rev


def degree_stats(degree_dict: dict) -> tuple[float, float, int, int, list[float]]:
    vals = list(degree_dict.values())
    if not vals:
        return 0.0, 0.0, 0, 0, []
    return (
        sum(vals) / len(vals),
        statistics.median(vals),
        max(vals),
        min(vals),
        statistics.quantiles(vals, n=5),
    )


def print_stats(out_deg: dict, in_deg: dict) -> None:
    o_avg, o_med, o_max, o_min, o_quint = degree_stats(out_deg)
    i_avg, i_med, i_max, i_min, i_quint = degree_stats(in_deg)
    print("Outgoing links:")
    print(f"  avg={o_avg:.4f}  median={o_med}  max={o_max}  min={o_min}  quintiles={o_quint}")
    print("Incoming links:")
    print(f"  avg={i_avg:.4f}  median={i_med}  max={i_max}  min={i_min}  quintiles={i_quint}")


def pagerank(
    adjacency_list: dict,
    reverse_adjacency: dict,
    all_ids: set,
    conv_threshold: float = 0.005,
) -> dict[int, float]:
    n = len(all_ids)
    ids_list = sorted(all_ids)
    pr = {i: 1.0 / n for i in ids_list}

    def out_degree(node):
        c = len(adjacency_list.get(node, []))
        return c if c > 0 else 1

    while True:
        new_pr = {}
        for a in ids_list:
            incoming = reverse_adjacency.get(a, [])
            contrib = sum(pr[t] / out_degree(t) for t in incoming)
            new_pr[a] = 0.15 / n + 0.85 * contrib
        total_old = sum(pr.values())
        total_new = sum(new_pr.values())
        if total_old == 0:
            break
        if abs(total_new - total_old) / total_old <= conv_threshold:
            break
        pr = new_pr
    return pr


def run_pipeline(all_ids: set, adjacency_list: dict) -> None:
    rev = build_reverse_adjacency(adjacency_list, all_ids)
    out_deg = {nid: len(adjacency_list.get(nid, [])) for nid in all_ids}
    in_deg = {nid: len(rev[nid]) for nid in all_ids}
    print_stats(out_deg, in_deg)
    pr = pagerank(adjacency_list, rev, all_ids)
    print("Top 5 by PageRank:")
    for pid, score in sorted(pr.items(), key=lambda x: -x[1])[:5]:
        print(f"  {pid}  {score:.6f}")


def run_test() -> None:
    # test a 3 node graph with cycle
    all_ids = {0, 1, 2}
    adjacency_list = {0: [1], 1: [2], 2: [0]}
    rev = build_reverse_adjacency(adjacency_list, all_ids)
    out_deg = {nid: len(adjacency_list.get(nid, [])) for nid in all_ids}
    in_deg = {nid: len(rev[nid]) for nid in all_ids}
    assert out_deg == {0: 1, 1: 1, 2: 1}
    assert in_deg == {0: 1, 1: 1, 2: 1}
    pr = pagerank(adjacency_list, rev, all_ids, conv_threshold=1e-9)
    assert abs(sum(pr.values()) - 1.0) < 1e-6
    for pid in all_ids:
        assert abs(pr[pid] - 1.0 / 3) < 1e-5
    print("All tests passed.")


def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument("--test", action="store_true")
    parser.add_argument("--data-dir", type=Path, default=Path("./web"))

    args = parser.parse_args()

    if args.test:
        run_test()
        return

    data_dir = args.data_dir
    if not data_dir.is_dir():
        print(f"Data directory not found: {data_dir}")
        return

    html_paths = sorted(data_dir.glob("*.html"))
    if not html_paths:
        print("No HTML files found.")
        return

    print(f"Found {len(html_paths)} HTML files, reading and parsing...")
    page_ids = {int(p.stem) for p in html_paths}

    def read_and_parse(path: Path) -> tuple[int, list[int]]:
        html = path.read_text(encoding="utf-8", errors="replace")
        return (int(path.stem), parse_html_links(html))

    items = []
    with ThreadPoolExecutor(max_workers=16) as executor:
        futures = {executor.submit(read_and_parse, p): p for p in html_paths}
        for i, future in enumerate(as_completed(futures), 1):
            items.append(future.result())
            if i % 1000 == 0:
                print(f"  {i}/{len(html_paths)} files")

    # Preserve order by page_id so build_graph is deterministic
    items.sort(key=lambda x: x[0])

    all_ids, adjacency_list = build_graph(items, page_ids)
    run_pipeline(all_ids, adjacency_list)


if __name__ == "__main__":
    main()
